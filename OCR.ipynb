{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.145 ðŸš€ Python-3.7.5 torch-1.13.1+cu117 CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "Setup complete âœ… (12 CPUs, 15.4 GB RAM, 128.0/233.7 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image, ImageOps\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "import re\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import rembg\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import easyocr\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from flask import Flask, request, jsonify\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arabic_OCR(img):\n",
    "\n",
    "        img_fr_or = img\n",
    "\n",
    "\n",
    "\n",
    "        ################# INPUT ##################\n",
    "\n",
    "        model = YOLO(\"version 2 best.pt\")\n",
    "        Fr_w = img_fr_or.copy()\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "\n",
    "        ############## REMOVE BACK GROUND ###############\n",
    "\n",
    "        def remove_background(img):\n",
    "            # Convert the input image to bytes\n",
    "            img = np.array(img)\n",
    "            s = img.copy()\n",
    "            # Step 3: Convert to RGB and swap red and blue channels manually\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Step 4: Convert back to PIL Image\n",
    "            img = Image.fromarray(img)\n",
    "            buffered = BytesIO()\n",
    "            img.save(buffered, format=\"PNG\")\n",
    "            encoded_image = buffered.getvalue()\n",
    "\n",
    "            # Remove the background\n",
    "            output_bytes = rembg.remove(encoded_image)\n",
    "\n",
    "            # Convert the output bytes back to an image\n",
    "            output_image = Image.open(BytesIO(output_bytes))\n",
    "\n",
    "            return output_image\n",
    "\n",
    "\n",
    "\n",
    "        ############## Perspective RATIO MODIFYING ###############\n",
    "\n",
    "\n",
    "        def rectify_id_card(img):\n",
    "            # Remove the background\n",
    "            img = np.array(remove_background(img))\n",
    "\n",
    "            # Convert the image to grayscale\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Change from BGR to RGB\n",
    "\n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Assuming the largest contour is the ID card\n",
    "            card_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "            # Find corners of the card\n",
    "            epsilon = 0.05 * cv2.arcLength(card_contour, True)\n",
    "            approx = cv2.approxPolyDP(card_contour, epsilon, True)\n",
    "\n",
    "            # Reorder the points to ensure they are in clockwise order\n",
    "            corners = np.array(approx).reshape(-1, 2)\n",
    "            ordered_corners = np.zeros_like(corners)\n",
    "\n",
    "            # Calculate the centroid of the points\n",
    "            centroid = np.mean(corners, axis=0)\n",
    "\n",
    "            # Sort the points based on their angle from the centroid\n",
    "            angles = np.arctan2(corners[:, 1] - centroid[1], corners[:, 0] - centroid[0])\n",
    "            sorted_indices = np.argsort(angles)\n",
    "\n",
    "            # Reorder the corners\n",
    "            for i in range(4):\n",
    "                ordered_corners[i] = corners[sorted_indices[i]]\n",
    "\n",
    "            # Define the coordinates of the corners of the desired rectangle\n",
    "            width, height = 840, 530\n",
    "            dst_corners = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype='float32')\n",
    "\n",
    "            # Calculate the perspective transform matrix\n",
    "            M = cv2.getPerspectiveTransform(ordered_corners.astype('float32'), dst_corners)\n",
    "\n",
    "            # Apply the perspective transformation\n",
    "            result = cv2.warpPerspective(img, M, (width, height))\n",
    "            result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "            return result\n",
    "        #####################################################\n",
    "\n",
    "        Fr_w = rectify_id_card(img_fr_or.copy())\n",
    "\n",
    "\n",
    "        # Initialize easyocr for Arabic and English\n",
    "        reader = easyocr.Reader(['fa', 'ar'], verbose=True)  # 'fa' is the code for Persian, which shares the script with Arabic.\n",
    "\n",
    "        def yolo_ID(img):\n",
    "            kernel = np.array([[-1,-1,-1],\n",
    "                              [-1, 9,-1],\n",
    "                              [-1,-1,-1]])\n",
    "\n",
    "            # Apply the convolution operation\n",
    "            img = cv2.filter2D(img, -1, kernel)\n",
    "            inverted_image = cv2.bitwise_not(img)\n",
    "            img = inverted_image\n",
    "            predict = model.predict(img, save=True, save_txt=True, conf=0.7)\n",
    "            outs = open(\"/home/mo/ocr_app/runs/detect/predict/labels/image0.txt\" , \"r\").readlines()\n",
    "            label = ''\n",
    "            for i in outs:\n",
    "              label = label+i\n",
    "\n",
    "            classes = ['0','1','2','3','4','5','6','7','8','9']\n",
    "            shutil.rmtree(\"/home/mo/ocr_app/runs\")\n",
    "            # Extract and sort classes based on x-coordinate\n",
    "            detected_entries = []\n",
    "            for line in label.strip().split(\"\\n\"):\n",
    "                class_id, x_coord, _, _, _ = map(float, line.split())\n",
    "                detected_entries.append((x_coord, classes[int(class_id)]))\n",
    "\n",
    "            detected_entries.sort(key=lambda x: x[0])  # Sort by x-coordinate\n",
    "\n",
    "            # Extract sorted detected classes from entries\n",
    "            detected_classes_sorted = [entry[1] for entry in detected_entries]\n",
    "\n",
    "            # Print detected classes\n",
    "            return ''.join(detected_classes_sorted)\n",
    "\n",
    "\n",
    "        def extract_arabic_english(text):\n",
    "            pattern = r'[a-zA-Z\\u0600-\\u06FF\\s]+'\n",
    "            matches = re.findall(pattern, text)\n",
    "            cleaned_text = ' '.join(matches)\n",
    "            return cleaned_text\n",
    "\n",
    "        def ocr_image(mask, reader):\n",
    "            results = reader.readtext(mask,text_threshold=0.7, low_text=0.3,paragraph = True)  # Set the allowlist to Arabic script\n",
    "            detected_texts = [item[1] for item in results]\n",
    "            full_text = ' '.join(detected_texts)\n",
    "            cleaned_text = extract_arabic_english(full_text)\n",
    "            return cleaned_text, results\n",
    "\n",
    "\n",
    "        def process_image(image, image_name='Image.jpg'):\n",
    "            # Define a sharpening kernel\n",
    "\n",
    "            img = image.copy()\n",
    "            img = cv2.resize(img, (840, 530))\n",
    "            alpha = 1\n",
    "            beta = 1\n",
    "            img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "            masks = {\n",
    "                \"name_mask\": img[140:240, 430:],\n",
    "                \"add_mask\": img[240:380, 450:],\n",
    "                \"Id_mask\": img[350:, 350:]\n",
    "            }\n",
    "            texted = []\n",
    "            # Reverse the order of processing masks\n",
    "            for mask_name, mask in (list(masks.items())):\n",
    "                cleaned_text, results = ocr_image(mask, reader)\n",
    "\n",
    "                cleaned_text = cleaned_text.split()\n",
    "                for tx in range(len(cleaned_text)):\n",
    "                  if cleaned_text[tx][:3] == 'Ù…Ø­Ù…':\n",
    "                    cleaned_text[tx] = 'Ù…Ø­Ù…Ø¯'\n",
    "\n",
    "                texted.append(cleaned_text)\n",
    "            return texted\n",
    "\n",
    "\n",
    "        # Process the first image\n",
    "        Name, Location, _= process_image(Fr_w)\n",
    "        nName = ' '\n",
    "        for n in Name:\n",
    "          nName = nName +' '+ n\n",
    "        nLocation = ' '\n",
    "        for n in Location:\n",
    "          nLocation = nLocation +' '+ n\n",
    "        ID_N = yolo_ID(Fr_w)\n",
    "        #print(nName, nLocation, ID_N, Job, Gender, Religion)\n",
    "\n",
    "        return ID_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "\n",
      "0: 416x640 2 0s, 3 1s, 3 2s, 1 4, 1 6, 2 7s, 1 9, 11356.2ms\n",
      "Speed: 51.6ms preprocess, 11356.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Results saved to \u001b[1m/home/mo/ocr_app/runs/detect/predict\u001b[0m\n",
      "1 label saved to /home/mo/ocr_app/runs/detect/predict/labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2921206140717\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "\n",
    "@app.route('/image', methods=['POST'])\n",
    "def post_example():\n",
    "    # Access request data using request object\n",
    "    data = request.json  # Assumes JSON data in the request body\n",
    "\n",
    "    # Return the received data in the response\n",
    "    return jsonify(data)\n",
    "\n",
    "@app.route('/upload_photo', methods=['POST'])\n",
    "def upload_photo():\n",
    "    if 'photo' not in request.files:\n",
    "        return jsonify({'message': 'No file part'}), 400\n",
    "\n",
    "    photo = request.files['photo']\n",
    "    if photo.filename == '':\n",
    "        return jsonify({'message': 'No selected file'}), 400\n",
    "\n",
    "    # You can process the file without saving it\n",
    "    # For example, print the file name and content type\n",
    "    print('Received File:', photo.filename)\n",
    "    print('Content Type:', photo.content_type)\n",
    "\n",
    "    return jsonify({'message': 'File received'})\n",
    "\n",
    "# paste here the code of ur machine learning model \n",
    "\n",
    "\n",
    "img = cv2.imread('test2.jpg')\n",
    "ID_N = Arabic_OCR(img)\n",
    "\n",
    "\n",
    "# print the result of the model\n",
    "print(ID_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
